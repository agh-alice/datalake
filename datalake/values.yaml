spark:
  master:
    image: nowickib/spark-master:latest
  executor:
    image: nowickib/spark-executor:latest

s3:
  endpoint: https://s3p.cloud.cyfronet.pl
  region: eu-central-1 # ignored when using Minio
  bucket: s3a://warehouse
  secret:
    name: s3-creds
    access_key_id: access_key_id
    secret_access_key: secret_access_key

nessie:
  versionStoreType: JDBC
  postgres:
    jdbcUrl: jdbc:postgresql://149.156.10.139:5433/nessie_data
    secret:
      name: nessie-postgres-creds
      username: username
      password: password

airflow:
  airflowVersion: 2.7.1
  images:
    airflow:
      pullPolicy: Always
      repository: nowickib/airflow
      tag: latest
  webserverSecretKeySecretName: airflow-webserver-secret
  webserver:
    startupProbe:
      periodSeconds: 120
    defaultUser:
      username: admin
      password: admin
      role: Public # we need to change password and role manually, setting public here, so default user doesn't have access to everything be default
  dags:
    persistence:
      enabled: false
    gitSync:
      enabled: true
      repo: https://github.com/PracaInzynierskaAlice/datalake.git
      branch: master
      subPath: 'src/dags'
  postgresql:
    enabled: false #turns off default postgres database
  data:
    metadataSecretName: airflow-metadata-secret
  pgbouncer:
    enabled: true

  env:
    - name: NESSIE_URL
      value: http://datalake-nessie:19120/api/v1
    - name: S3_ENDPOINT
      value: https://s3p.cloud.cyfronet.pl
    - name: S3_REGION
      value: eu-central-1
  
  secret:
    - envName: S3_ACCESS_KEY_ID
      secretName: s3-creds
      secretKey: access_key_id
    - envName: S3_SECRET_ACCESS_KEY
      secretName: s3-creds
      secretKey: secret_access_key

dremio:
  coordinator:
    cpu: 2
    memory: 4096
    count: 0
    volumeSize: 1Gi

  executor:
    cpu: 1
    memory: 4096
    count: 2
    volumeSize: 0
    cloudCache:
      enabled: false

  zookeeper:
    cpu: 0.5
    memory: 1024
    count: 3
    volumeSize: 512Mi

  distStorage:
    type: "aws"
    aws:
      bucketName: "alice-data-lake-dev"
      path: "/dremio"
      authentication: "accessKeySecret"

      credentials:
        accessKey: access_key_id
        secret: secret_access_key

      extraProperties: |
        <property>
          <name>fs.s3a.path.style.access</name>
          <value>true</value>
        </property>
        <property>
          <name>fs.s3a.endpoint</name>
          <value>s3p.cloud.cyfronet.pl</value>
        </property>
        <property>
          <name>dremio.s3.compat</name>
          <value>true</value>
        </property>
